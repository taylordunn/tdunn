---
title: "Predicting bike ridership: developing a model"
description: |
  Part 2 of predicting bike ridership in Halifax, Nova Scotia. In this post,
  I explore and tune different modeling approaches.
author:
  - name: Taylor Dunn
date: 2022-04-29
params:
  date: 2022-04-29
  slug: 
categories:
  - R
  - machine learning
output:
  distill::distill_article:
    self_contained: false
    toc: true
draft: true
---

```{r setup, include=TRUE, code_folding="Setup"}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(gt)
library(patchwork)
library(tidymodels)

library(dunnr)
extrafont::loadfonts(device = "win", quiet = TRUE)
theme_set(theme_td())
set_geom_fonts()
set_palette()
```

## Introduction

Import the data from part 1:

```{r}
bike_ridership <- read_rds(
  "../2022-04-27-predicting-bike-ridership-getting-the-data/bike-ridership-data.rds"
)
```

## Exploratory data analysis

```{r}
glimpse(bike_ridership)
```

The `n_bikes` variable here is the total number of bikes counted at `site_name` over `count_date`.
In the original data, counts are recorded every hour which is reflected by the `n_records` variable:

```{r}
bike_ridership %>%
  count(site_name, n_records) %>%
  gt()
```

All except the Hollis St site have two channels (northbound and southbound), which is why `n_records` = 48. The entries with fewer `n_records` reflect the time of day that the data was extracted:

```{r}
bike_ridership %>%
  filter(n_records < 24) %>%
  select(site_name, count_date, n_records) %>%
  gt()
```

For this analysis, I will exclude this incomplete day:

```{r}
bike_ridership <- bike_ridership %>% filter(n_records > 8)
```

The date ranges for each site:

```{r fig.height=2, fig.width=5}
bike_ridership %>%
  group_by(site_name) %>%
  summarise(
    min_date = min(count_date), max_date = max(count_date),
    n_days = n(), .groups = "drop"
  ) %>%
  mutate(
    site_name = fct_reorder(site_name, min_date),
    n_days_label = ifelse(site_name == levels(site_name)[1],
                          str_c("n_days = ", n_days), n_days),
    midpoint_date = min_date + n_days / 2
  ) %>%
  ggplot(aes(y = site_name, color = site_name)) +
  geom_linerange(aes(xmin = min_date, xmax = max_date)) +
  geom_point(aes(x = min_date)) +
  geom_point(aes(x = max_date)) +
  geom_text(aes(label = n_days_label, x = midpoint_date), vjust = -0.5) +
  scale_y_discrete(labels = ~ str_wrap(., width = 15)) +
  labs(y = NULL, x = "min_date -> max_date") +
  theme(legend.position = "none")
```

For each site, the distribution of daily `n_bikes`:

```{r}
bike_ridership %>%
  ggplot(aes(x = n_bikes, fill = site_name)) +
  geom_histogram(bins = 30) +
  facet_wrap(~ str_trunc(site_name, 15)) +
  theme(legend.position = "none")
```

The South Park St site appears bimodal, which [I noted in part 1](../2022-04-27-predicting-bike-ridership-getting-the-data) was likely due to the addition of protected bike lanes in 2021.
This can be seen more clearly in the trend over time:

```{r fig.height=5, fig.width=6}
bike_ridership %>%
  ggplot(aes(x = count_date, y = n_bikes, color = site_name)) +
  geom_line() +
  facet_wrap(~ site_name, ncol = 1) +
  theme(legend.position = "none")
```

As you would expect for data in the same city, bike counters between sites are very highly correlated, which I can visualize:

```{r}
bike_ridership %>%
  transmute(count_date, n_bikes1 = n_bikes,
            site_name1 = factor(str_trunc(site_name, 10))) %>%
  left_join(., rename(., n_bikes2 = n_bikes1, site_name2 = site_name1),
            by = "count_date") %>%
  filter(as.numeric(site_name1) < as.numeric(site_name2)) %>%
  ggplot(aes(x = n_bikes1, y = n_bikes2)) +
  geom_point(aes(color = site_name1), alpha = 0.3) +
  facet_grid(site_name2 ~ site_name1) +
  theme(legend.position = "none") +
  dunnr::add_facet_borders()
```

The weather variables have varying levels of completeness:

```{r fig.height=2.5, fig.width=5}
# Separate out the weather data
weather_data <- bike_ridership %>%
  distinct(count_date, mean_temperature, total_precipitation,
           speed_max_gust, snow_on_ground)

weather_data %>%
  mutate(across(where(is.numeric), is.na)) %>%
  pivot_longer(cols = -count_date) %>%
  ggplot(aes(x = count_date, y = name)) +
  geom_tile(aes(fill = value)) +
  labs(y = NULL, x = NULL, fill = "Missing") +
  scale_fill_manual(values = c(td_colors$nice$indigo_blue, "gray80")) +
  scale_x_date(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0)) +
  theme(legend.position = "top")
```

The distributions:

```{r fig.height=3, fig.width=6}
weather_data %>%
  pivot_longer(cols = -count_date) %>%
  filter(!is.na(value)) %>%
  ggplot(aes(x = value, fill = name)) +
  geom_histogram(bins = 30) +
  facet_wrap(~ name, nrow = 1) +
  theme(legend.position = "none")
```

For non-missing cases, plot the pairwise relationships:

```{r}
weather_data %>%
  pivot_longer(cols = -count_date, names_to = "var1", values_to = "val1") %>%
  mutate(var1 = factor(var1)) %>%
  left_join(., rename(., var2 = var1, val2 = val1),
            by = "count_date") %>%
  filter(!is.na(val1), !is.na(val2),
         # Use numeric factor labels to remove duplicates
         as.numeric(var1) < as.numeric(var2)) %>%
  ggplot(aes(x = val1, y = val2, color = var1)) +
  geom_point(alpha = 0.5) +
  facet_grid(var2 ~ var1, scales = "free") +
  theme(legend.position = "none") +
  dunnr::add_facet_borders()
```

Unsurprisingly, the clearest relationship in these data is decreasing `snow_on_ground` with increasing `mean_temperature` (top left plot).

Visualize relationships with `n_bikes`:

```{r}
bike_ridership %>%
  pivot_longer(
    cols = c(mean_temperature, total_precipitation,
             speed_max_gust, snow_on_ground),
    names_to = "var", values_to = "val"
  ) %>%
  filter(!is.na(val)) %>%
  ggplot(aes(x = val, y = n_bikes)) +
  geom_point(aes(color = str_trunc(site_name, 15)), alpha = 0.4) +
  facet_wrap(~ var, nrow = 2, scales = "free_x") +
  dunnr::add_facet_borders() +
  labs(x = NULL, color = NULL) +
  theme(legend.position = "bottom")
```

All of the weather variables seem to be associated with `n_bikes`.
In terms of predictive value, `mean_temperature` looks like it might be the most useful, and `speed_max_gust` the least.

## Feature engineering

From my EDA, I decided I want try including all 4 weather variables to predict bike ridership.
For some of the machine learning models I plan to try (like basic linear regression), this will require imputation of missing values, which I'll attempt here.

Add some more time variables for working with the `weather_data`:

```{r}
weather_data <- weather_data %>%
  mutate(count_year = lubridate::year(count_date),
         # Day number relative to earliest date
         count_day = as.numeric(count_date - min(count_date)),
         # Day of the year, from 1 to 365
         count_yday = lubridate::yday(count_date))
```

### Temperature

The `mean_temperature` variable is missing `r scales::percent(mean(is.na(weather_data$mean_temperature)))` of values.
Visualize the trend over time:

```{r}
p1 <- weather_data %>%
  filter(!is.na(mean_temperature)) %>%
  ggplot(aes(x = count_date, y = mean_temperature)) +
  geom_line(aes(color = factor(count_year))) +
  scale_color_viridis_d("year") +
  scale_x_date("date", date_breaks = "1 year")
p2 <- weather_data %>%
  filter(!is.na(mean_temperature)) %>%
  ggplot(aes(x = count_yday, y = mean_temperature)) +
  geom_line(aes(color = factor(count_year))) +
  scale_color_viridis_d("year") +
  scale_x_continuous("day of year", breaks = c(0, 90, 180, 270, 365)) +
  labs(y = NULL)
p1 + p2 +
  plot_layout(guides = "collect") &
  theme(legend.position = "top")
```

The cyclic nature makes it a good candidate for smoothing splines.
As a starting point, try a natural spline with 5 knots on the `count_yday` variable:

```{r}
library(splines)
lm_temperature <- 
  lm(mean_temperature ~ ns(count_yday, knots = 5),
      data = filter(weather_data, !is.na(mean_temperature)))

p1 +
  geom_line(
    data = augment(lm_temperature, newdata = weather_data),
    aes(y = .fitted), size = 1
  ) +
  theme(legend.position = "top")
```

We can obviously do a lot better.
I'll fit the data using a generalized additive model (GAM) with the `mgcv` package.^[Check out [this blog post by Gavin Simpson ](https://fromthebottomoftheheap.net/2014/05/09/modelling-seasonal-data-with-gam/) for a great walkthrough of modeling seasonal data with GAMs.]
For the `count_yday` variable (ranges from 1-365), I'll make sure that there is no discontinuity between year by using a *cyclic* spline (`bs = "cc"`).
I'll also include a smoothing term of `count_day` which will capture the trend across years.

```{r}
library(mgcv)

gam_temperature <-
  gam(mean_temperature ~ s(count_yday, bs = "cc", k = 12) + s(count_day),
      data = filter(weather_data, !is.na(mean_temperature)))
plot(gam_temperature, pages = 1, shade = TRUE)
```

The left plot shows the seasonal trend within a year (note the lines would connect at `count_yday` = 1 and 365), and the right plot shows the increase in average temperature throughout time (across years) after accounting for the seasonal effect.
Overlay the fit:

```{r}
p1 +
  geom_line(
    data = augment(gam_temperature, newdata = weather_data),
    aes(y = .fitted), size = 1
  ) +
  theme(legend.position = "top")
```

It doesn't capture some of the coldest temperatures, but I'm pretty happy with that.
I'll use predictions from the GAM model to impute missing days:

```{r}
weather_data <- weather_data %>%
  bind_cols(pred = predict(gam_temperature, newdata = weather_data)) %>%
  mutate(mean_temperature = ifelse(is.na(mean_temperature), pred,
                                   mean_temperature)) %>%
  select(-pred)
```

### Precipitation and snow

The `total_precipitation` variable is missing
`r scales::percent(mean(is.na(weather_data$total_precipitation)))` of values;
`r scales::percent(mean(is.na(weather_data$snow_on_ground)))` for `snow_on_ground`.

The `total_precipitation` distribution:

```{r}
p1 <- weather_data %>%
  mutate(total_precipitation = replace_na(total_precipitation, -5)) %>%
  ggplot(aes(x = count_date, y = total_precipitation)) +
  geom_point(alpha = 0.5) +
  scale_y_continuous(breaks = c(-5, 0, 20, 40),
                     labels = c("missing", 0, 20, 40))
p1
```

This pattern of missing data during winter months makes me think that the `total_precipitation` is actually total rainfall, i.e. snowfall is not counted.
I'm going to impute the missing values with 0 during pre-processing, which is admittedly a poor approximation of the truth -- I know first-hand that there has been some rainy days in Halifax during April 2022, for example.

The `snow_on_ground` distribution:

```{r}
p2 <- weather_data %>%
  mutate(snow_on_ground = replace_na(snow_on_ground, -2)) %>%
  ggplot(aes(x = count_date, y = snow_on_ground)) +
  geom_point(alpha = 0.5) +
  scale_y_continuous(breaks = c(-2, 0, 10, 20),
                     labels = c("missing", 0, 10, 20))
p2
```

I'll take the same zero imputation approach here, which I'm a lot more confident doing here because most of the missing values occur during non-winter months.
A more careful approach might involve imputing 0 during non-winter months that I'm certain would have no snow on the ground, then modeling the winter months with something like a zero-inflated Poisson model.

### Wind speed

The `speed_max_gust` variable is the daily maximum wind speed in km/h, and has
`r scales::percent(mean(is.na(weather_data$speed_max_gust)))` missing values.

```{r}
weather_data %>%
  ggplot(aes(x =))
```


```{r}
d <- weather_data %>%
  select(count_date, total_rainfall, snow_on_ground,
         count_year, count_yday, count_day)
d %>%
  count(snow_on_ground, sort = T)
d %>%
  count(total_rainfall, sort = T)
```


## Reproducibility {.appendix}

<details><summary>Session info</summary>

```{r echo=FALSE}
devtools::session_info()$platform
devtools::session_info()$packages %>%
  rmarkdown::paged_table()
```

</details>

<details><summary>Git repository</summary>

```{r echo=FALSE}
git2r::repository()
```

</details>

```{r echo=FALSE}
dunnr::get_distill_source(date = params$date, slug = params$slug)
```
